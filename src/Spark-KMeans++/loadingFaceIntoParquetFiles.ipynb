{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cluster=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access s3 files, the first step is setting AWS credential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if Cluster:\n",
    "    %run Credentials.ipynb\n",
    "\n",
    "    # stop sc before restarting it.\n",
    "    #sc.stop()\n",
    "\n",
    "    from pyspark import SparkContext,SparkConf\n",
    "    sparkConfig=SparkConf()\n",
    "    sparkConfig.set(\"spark.executor.memory\",\"20g\")\n",
    "    sparkConfig.set(\"spark.worker.memory\",\"20g\")\n",
    "    sparkConfig.set(\"spark.driver.cores\",\"8\")\n",
    "    sparkConfig.set(\"spark.python.worker.memory\",\"20g\")\n",
    "    sparkConfig.getAll()\n",
    "\n",
    "    sc=SparkContext(conf=sparkConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test sc\n",
    "RDD=sc.parallelize(range(100))\n",
    "RDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tarnames= ['Bernie.tgz']\n",
      "/Users/yoavfreund/projects/DimensionalityEstimation/src/Spark-KMeans++\n",
      "x data1/output/Bernie Sanders Promposals-Ks2v3LSd29U_windows0.pkl\n",
      "x data1/output/Bernie Sanders Promposals-Ks2v3LSd29U_windows1.pkl\n",
      "x data1/output/Bernie Sanders Promposals-Ks2v3LSd29U_windows2.pkl\n",
      "x data1/output/Bernie Sanders Promposals-Ks2v3LSd29U_windows3.pkl\n",
      "x data1/output/Bernie Sanders Promposals-Ks2v3LSd29U_windows4.pkl\n",
      "x data1/output/Bernie Sanders Promposals-Ks2v3LSd29U_windows5.pkl\n",
      "Bernie Sanders Promposals-Ks2v3LSd29U_windows0.pkl\n",
      "Bernie Sanders Promposals-Ks2v3LSd29U_windows1.pkl\n",
      "Bernie Sanders Promposals-Ks2v3LSd29U_windows2.pkl\n",
      "Bernie Sanders Promposals-Ks2v3LSd29U_windows3.pkl\n",
      "Bernie Sanders Promposals-Ks2v3LSd29U_windows4.pkl\n",
      "Bernie Sanders Promposals-Ks2v3LSd29U_windows5.pkl\n"
     ]
    }
   ],
   "source": [
    "if Cluster:\n",
    "    s3helper.set_credential(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "    # Then open the bucket that has your files.\n",
    "    s3helper.open_bucket('yoav-faces')\n",
    "\n",
    "    # Now you can list your files in the bucket.\n",
    "\n",
    "    print s3helper.ls()\n",
    "    tarnames=s3helper.ls('output/')\n",
    "    print 'tarnames=',tarnames[:10]\n",
    "    \n",
    "    !mkdir /mnt/output\n",
    "    %cd /mnt/output\n",
    "    !ls -lrt data1/output\n",
    "    !rm data1/output/*.pkl\n",
    "\n",
    "    workdir='/mnt/output/data1/output/'\n",
    "    %cd '/mnt/output'\n",
    "\n",
    "else:  #run on local\n",
    "    tarnames=['Bernie.tgz']\n",
    "    print 'tarnames=',tarnames\n",
    "\n",
    "    workdir='/Users/yoavfreund/projects/DimensionalityEstimation/src/Spark-KMeans++/data1/output/'\n",
    "    %cd '/Users/yoavfreund/projects/DimensionalityEstimation/src/Spark-KMeans++'\n",
    "    !cp /Users/yoavfreund/projects/DimensionalityEstimation/output/Bernie.tgz .\n",
    "    !tar -xzvf Bernie.tgz\n",
    "    !ls $workdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"\"\"Code for packing and unpacking a numpy array into a byte array.\n",
    "   the array is flattened if it is not 1D.\n",
    "   This is intended to be used as the interface for storing \n",
    "   \n",
    "   This code is intended to be used to store numpy array as fields in a dataframe and then store the \n",
    "   dataframes in a parquet file.\n",
    "\"\"\"\n",
    "\n",
    "def packArray(a):\n",
    "    if type(a)!=np.ndarray:\n",
    "        raise Exception(\"input to packArray should be numpy.ndarray. It is instead \"+str(type(a)))\n",
    "    return bytearray(a.tobytes())\n",
    "def unpackArray(x,data_type=np.int16):\n",
    "    return np.frombuffer(x,dtype=data_type)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Defining schema explicitly seems to only cause problems.\n",
    "\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType, BinaryType\n",
    "schema = StructType([\n",
    "            StructField(\"video_num\", IntegerType(), False),\n",
    "            StructField(\"track_num\", IntegerType(), False),\n",
    "            StructField(\"frame_num\", IntegerType(), False),\n",
    "            StructField(\"ulx\", IntegerType(), False),\n",
    "            StructField(\"uly\", IntegerType(), False),\n",
    "            StructField(\"size\", IntegerType(), False),\n",
    "            StructField(\"image\", BinaryType(), False)])\n",
    "schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running in local mode 0 Bernie.tgz\n",
      "Tue Aug  9 08:58:38 PDT 2016\n",
      "0 994 0 6 994\n",
      "1 846 1 6 1840\n",
      "2 228 2 6 2068\n",
      "3 101 3 6 2169\n",
      "4 254 4 6 2423\n",
      "5 106 5 6 2529\n",
      "size of data= 20304\n",
      "[1.1028499603271484, 0.9427151679992676, 0.2554168701171875, 0.1090090274810791, 0.2841639518737793, 0.11392402648925781, 3.166512966156006, 1.3507020473480225, 9.4113130569458]\n",
      "finished  Bernie.tgz 0 added 2529 for a total of 2529\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "from pyspark.sql import Row, SQLContext\n",
    "import sys,traceback\n",
    "from glob import glob\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from time import time\n",
    "from pyspark import StorageLevel\n",
    "sqlC=SQLContext(sc)\n",
    "\n",
    "ALL=[]\n",
    "cumul=0\n",
    "\n",
    "for tar_index in range(len(tarnames)):\n",
    "    tar_path=tarnames[tar_index]\n",
    "    if Cluster:\n",
    "        tarname=tar_path[7:]\n",
    "        print 'starting on ',tar_index,tarname\n",
    "    else:\n",
    "        tarname=tar_path\n",
    "        print 'Running in local mode',tar_index,tarname\n",
    "    !date\n",
    "    try:\n",
    "        if Cluster:\n",
    "            s3helper.get_file(tar_path)\n",
    "\n",
    "            tarname=tar_path\n",
    "            !ls -l \"$tarname\"\n",
    "            !tar -xzvf \"$tarname\"\n",
    "            !rm \"$tarname\"\n",
    "            !mv *.pkl $workdir\n",
    "        \n",
    "        video_names={}\n",
    "        video_index=0\n",
    "\n",
    "        pattern=re.compile(r'.*/([^/]+)_windows(\\d+)\\.pkl')\n",
    "\n",
    "        List=glob(workdir+'*')\n",
    "        data=[]\n",
    "        ts=[]\n",
    "\n",
    "        for file in List:\n",
    "            ts.append(time())\n",
    "            match=re.search(pattern,file)\n",
    "            if match:\n",
    "                video_name=match.group(1)\n",
    "                if not video_name in video_names.keys():        \n",
    "                    video_names[video_name]=video_index\n",
    "                    video_index+=1\n",
    "                video_num=video_names[video_name]\n",
    "                window_num=int(match.group(2))\n",
    "            else:\n",
    "                print 'COULD NOT FIND NUMBER IN',file\n",
    "                continue\n",
    "\n",
    "            In = pickle.load(open(file,'r'))\n",
    "            print window_num,len(In),\n",
    "            Full=[]\n",
    "            for f in In:\n",
    "                Lim=packArray(np.array(f[-1],dtype=np.int16))\n",
    "                row=Row(video_num=int(video_num), track_num=int(window_num),frame_num=int(f[0]),\n",
    "                        ulx=int(f[1]),uly=int(f[2]),size=int(f[3]),image=Lim)\n",
    "                Full.append(row)\n",
    "                \n",
    "            In=[]\n",
    "            data=data+Full\n",
    "            Full=[]\n",
    "            print window_num,len(List),len(data)\n",
    "            #break\n",
    "            \n",
    "        print 'size of data=',getsizeof(data)\n",
    "\n",
    "        ts.append(time())\n",
    "        rdd=sc.parallelize(data,10)\n",
    "        ts.append(time())\n",
    "        New=sqlC.createDataFrame(rdd).persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "        ts.append(time())\n",
    "        _size=New.count()\n",
    "        ts.append(time())\n",
    "        ALL.append(New)\n",
    "        \n",
    "        print [ts[i+1]-ts[i] for i in range(len(ts)-1)]\n",
    "        cumul += _size\n",
    "        print 'finished ',tarname,\n",
    "        print tar_index,'added',_size,'for a total of',cumul\n",
    "        print '='*70\n",
    "        break\n",
    "\n",
    "    except:\n",
    "        print \"Exception while reading file:\",file\n",
    "        print '-'*60\n",
    "        traceback.print_exc(file=sys.stdout)\n",
    "        print '-'*60\n",
    "\n",
    "    !rm /mnt/output/data1/output/*.pkl\n",
    "    !rm *.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "2529\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#New=sqlC.createDataFrame(rdd)\n",
    "print type(New)\n",
    "print New.count()\n",
    "LX=New.take(5)\n",
    "for X in LX:\n",
    "    print X.frame_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 240488\r\n",
      "-rw-r--r--  1 yoavfreund  staff         0 Aug  9 08:25 _SUCCESS\r\n",
      "-rw-r--r--  1 yoavfreund  staff       668 Aug  9 08:25 _common_metadata\r\n",
      "-rw-r--r--  1 yoavfreund  staff   3609419 Aug  9 08:25 _metadata\r\n",
      "-rw-r--r--  1 yoavfreund  staff  10241699 Aug  9 08:25 part-r-00000-15f372be-80f1-4ad0-9e4c-1dfcc7a6f1c4.gz.parquet\r\n",
      "-rw-r--r--  1 yoavfreund  staff  10529844 Aug  9 08:25 part-r-00001-15f372be-80f1-4ad0-9e4c-1dfcc7a6f1c4.gz.parquet\r\n",
      "-rw-r--r--  1 yoavfreund  staff  11369446 Aug  9 08:25 part-r-00002-15f372be-80f1-4ad0-9e4c-1dfcc7a6f1c4.gz.parquet\r\n",
      "-rw-r--r--  1 yoavfreund  staff  10930243 Aug  9 08:25 part-r-00003-15f372be-80f1-4ad0-9e4c-1dfcc7a6f1c4.gz.parquet\r\n",
      "-rw-r--r--  1 yoavfreund  staff  10834174 Aug  9 08:25 part-r-00004-15f372be-80f1-4ad0-9e4c-1dfcc7a6f1c4.gz.parquet\r\n",
      "-rw-r--r--  1 yoavfreund  staff  10959497 Aug  9 08:25 part-r-00005-15f372be-80f1-4ad0-9e4c-1dfcc7a6f1c4.gz.parquet\r\n",
      "-rw-r--r--  1 yoavfreund  staff  10870156 Aug  9 08:25 part-r-00006-15f372be-80f1-4ad0-9e4c-1dfcc7a6f1c4.gz.parquet\r\n",
      "-rw-r--r--  1 yoavfreund  staff  13922946 Aug  9 08:25 part-r-00007-15f372be-80f1-4ad0-9e4c-1dfcc7a6f1c4.gz.parquet\r\n",
      "-rw-r--r--  1 yoavfreund  staff  15007487 Aug  9 08:25 part-r-00008-15f372be-80f1-4ad0-9e4c-1dfcc7a6f1c4.gz.parquet\r\n",
      "-rw-r--r--  1 yoavfreund  staff  14830771 Aug  9 08:25 part-r-00009-15f372be-80f1-4ad0-9e4c-1dfcc7a6f1c4.gz.parquet\r\n"
     ]
    }
   ],
   "source": [
    "parquet_file=\"TestFaces.parquet\"\n",
    "!rm -rf $parquet_file\n",
    "New.write.save(parquet_file)\n",
    "\n",
    "!ls -l $parquet_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ulx=567, uly=228),\n",
       " Row(ulx=566, uly=227),\n",
       " Row(ulx=565, uly=225),\n",
       " Row(ulx=564, uly=223),\n",
       " Row(ulx=565, uly=222)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.sql(\"SELECT ulx,uly FROM parquet.`%s`\"%parquet_file)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3helper.open_bucket(\"mas-dse-public\")\n",
    "\n",
    "files = s3helper.load_path('/Weather/US_Weather.parquet', '/US_Weather.parquet')\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = SparkContext(master=master_url)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.sql(\"SELECT station, measurement FROM parquet.`/US_Weather.parquet`\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
